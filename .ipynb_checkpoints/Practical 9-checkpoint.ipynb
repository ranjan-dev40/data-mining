{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csV_uZ04AiXE"
   },
   "source": [
    "# **Data Features Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60V-ib82AtH7"
   },
   "source": [
    "**Importance of Data Feature Selection**<br>\n",
    "The performance of machine learning model is directly proportional to the data features used to train it. The performance of ML model will be affected negatively if the data features provided to it are irrelevant. On the other hand, use of relevant data features can increase the accuracy of your ML model especially linear and logistic regression.\n",
    "\n",
    "Now the question arise that what is automatic feature selection? It may be defined as the process with the help of which we select those features in our data that are most relevant to the output or prediction variable in which we are interested. It is also called attribute selection.\n",
    "\n",
    "The following are some of the benefits of automatic feature selection before modeling the data −\n",
    "\n",
    "1. Performing feature selection before data modeling will reduce the overfitting.\n",
    "2. Performing feature selection before data modeling will increases the accuracy of ML model.\n",
    "3. Performing feature selection before data modeling will reduce the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgY3m83rBRAv"
   },
   "source": [
    "**Feature Selection Techniques**<br>\n",
    "The followings are automatic feature selection techniques that we can use to model ML data in Python −"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bnx9tQqBTJU"
   },
   "source": [
    "**Univariate Selection**<br>\n",
    "This feature selection technique is very useful in selecting those features, with the help of statistical testing, having strongest relationship with the prediction variables. We can implement univariate feature selection technique with the help of SelectKBest0class of scikit-learn Python library.\n",
    "\n",
    "**Example**<br>\n",
    "In this example, we will use Pima Indians Diabetes dataset to select 4 of the attributes having best features with the help of chi-square statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DdwYKYIKIS1i"
   },
   "outputs": [],
   "source": [
    "# Code to read csv file into colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oPfaNY6UITlF"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m1J6DYnuJcz9"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':'1-lM7lDTtTWgnMlbH5Tv4M2qn7FRTsGSy'}) # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "spSKXTTrBpI3",
    "outputId": "a267b38d-3860-4377-9fc8-f30407f17973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52 1411.89   17.61   53.11 2175.57  127.67    5.39  181.3 ]\n",
      "\n",
      "Featured data:\n",
      " [[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]]\n"
     ]
    }
   ],
   "source": [
    "#Driver Code\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('pima-indians-diabetes.csv')\n",
    "array = dataframe.values\n",
    "\n",
    "#Next, we will separate array into input and output components −\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#The following lines of code will select the best features from dataset −\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X,Y)\n",
    "\n",
    "#We can also summarize the data for output as per our choice. \n",
    "#Here, we are setting the precision to 2 and showing the 4 data \n",
    "#attributes with best features along with best score of each attribute −\n",
    "set_printoptions(precision=2)\n",
    "print(fit.scores_)\n",
    "featured_data = fit.transform(X)\n",
    "print (\"\\nFeatured data:\\n\", featured_data[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2ZDQ85FLeBy"
   },
   "source": [
    "**Recursive Feature Elimination**<br>\n",
    "As the name suggests, RFE (Recursive feature elimination) feature selection technique removes the attributes recursively and builds the model with remaining attributes. We can implement RFE feature selection technique with the help of RFE class of scikit-learn Python library.\n",
    "\n",
    "**Example**<br>\n",
    "In this example, we will use RFE with logistic regression algorithm to select the best 3 attributes having the best features from Pima Indians Diabetes dataset to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ur4gmLqnL6A2",
    "outputId": "0217da56-8ecf-45d3-bbc8-d7f5311d09ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('pima-indians-diabetes.csv')\n",
    "array = dataframe.values\n",
    "\n",
    "#Next, we will separate the array into its input and output components −\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#The following lines of code will select the best features from a dataset −\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"Number of Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyuCYrrnMPLm"
   },
   "source": [
    "We can see in above output, RFE choose preg, mass and pedi as the first 3 best features. They are marked as 1 in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzuLHgD2OKbD"
   },
   "source": [
    "**Principal Component Analysis (PCA)**<br>\n",
    "PCA, generally called data reduction technique, is very useful feature selection technique as it uses linear algebra to transform the dataset into a compressed form. We can implement PCA feature selection technique with the help of PCA class of scikit-learn Python library. We can select number of principal components in the output.\n",
    "\n",
    "<br>**Example**<br>\n",
    "In this example, we will use PCA to select best 3 Principal components from Pima Indians Diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Ul-IHIfUOhd8",
    "outputId": "f9b05c35-0d20-41ee-cd35-db390393625d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.89 0.06 0.03]\n",
      "[[-2.02e-03  9.78e-02  1.61e-02  6.08e-02  9.93e-01  1.40e-02  5.37e-04\n",
      "  -3.56e-03]\n",
      " [-2.26e-02 -9.72e-01 -1.42e-01  5.79e-02  9.46e-02 -4.70e-02 -8.17e-04\n",
      "  -1.40e-01]\n",
      " [-2.25e-02  1.43e-01 -9.22e-01 -3.07e-01  2.10e-02 -1.32e-01 -6.40e-04\n",
      "  -1.25e-01]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('pima-indians-diabetes.csv')\n",
    "array = dataframe.values\n",
    "\n",
    "#Next, we will separate array into input and output components −\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#The following lines of code will extract features from dataset −\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "print(\"Explained Variance: {}\".format(fit.explained_variance_ratio_))\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOivwyMLQGQ-"
   },
   "source": [
    "We can observe from the above output that 3 Principal Components bear little resemblance to the source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpUQhCVwQH-H"
   },
   "source": [
    "**Feature Importance**<br>\n",
    "As the name suggests, feature importance technique is used to choose the importance features. It basically uses a trained supervised classifier to select features. We can implement this feature selection technique with the help of ExtraTreeClassifier class of scikit-learn Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXSm9J4MQNDX"
   },
   "source": [
    "**Example**<br>\n",
    "In this example, we will use ExtraTreeClassifier to select features from Pima Indians Diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mFETt2slQTPf",
    "outputId": "6fdba490-fbf5-46d1-b747-21350a9d4ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11 0.23 0.1  0.08 0.08 0.14 0.12 0.14]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "path = r'C:\\Desktop\\pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('pima-indians-diabetes.csv')\n",
    "array = dataframe.values\n",
    "\n",
    "#Next, we will separate array into input and output components −\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#The following lines of code will extract features from dataset −\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kur31oTwQpst"
   },
   "source": [
    "From the output, we can observe that there are scores for each attribute. The higher the score, higher is the importance of that attribute.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
